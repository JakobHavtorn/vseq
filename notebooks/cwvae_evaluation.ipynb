{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "import scipy.stats\n",
    "\n",
    "from vseq.data import BaseDataset\n",
    "from vseq.data.batchers import AudioBatcher, ListBatcher\n",
    "from vseq.data.datapaths import DATASETS, TIMIT\n",
    "from vseq.data.loaders import AudioLoader, TIMITAlignmentLoader, TIMITSpeakerLoader\n",
    "from vseq.data.transforms import MuLawEncode, MuLawDecode\n",
    "from vseq.models.clockwork_vae import CWVAEAudioTasNet\n",
    "from vseq.settings import CHECKPOINT_DIR\n",
    "from vseq.utils.device import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1xrnjn5y | Big V100 model, 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"1xrnjn5y\"\n",
    "dataset = DATASETS[TIMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_path(run_id, checkpoint_dir=CHECKPOINT_DIR):\n",
    "    run_dirs = os.listdir(checkpoint_dir)\n",
    "    run_dir = [run_dir for run_dir in run_dirs if run_id in run_dir]\n",
    "\n",
    "    if len(run_dir) > 1:\n",
    "        raise IOError(f\"More than one run found with ID {run_id}: {run_dir}\")\n",
    "    elif len(run_dir) == 0:\n",
    "        raise IOError(f\"No runs found with ID {run_id}\")\n",
    "    return os.path.join(checkpoint_dir, run_dir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_mixture(means, log_scales, unnorm_weights, x_min=-1, x_max=1, n_vals=100, ax=None):\n",
    "    l = np.linspace(x_min, x_max, n_vals)\n",
    "    pdf = np.zeros_like(l)\n",
    "\n",
    "    weights = unnorm_weights.softmax(-1).cpu().numpy()\n",
    "    means = means.cpu().numpy()\n",
    "    variances = log_scales.exp().cpu().numpy()\n",
    "\n",
    "    for i in range(10):\n",
    "        pdf += scipy.stats.logistic.pdf(l, loc=means[i], scale=variances[i]) * weights[i]\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "    else:\n",
    "        fig = plt.gcf()\n",
    "\n",
    "    ax.plot(l, pdf)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = get_run_path(run_id)\n",
    "run_files_path = os.path.join(run_path, \"files\")\n",
    "run_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_free_gpus()\n",
    "model = CWVAEAudioTasNet.load(run_files_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary(input_size=(4, model.overall_stride), x_sl=torch.tensor([model.overall_stride]), device=\"cpu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_loader = AudioLoader(\"wav\", cache=False)\n",
    "audio_batcher = AudioBatcher(padding_module=model.overall_stride)\n",
    "transform_enc = MuLawEncode(bits=16)\n",
    "transform_dec = MuLawDecode(bits=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = [\n",
    "    (audio_loader, transform_enc, audio_batcher),\n",
    "    (TIMITAlignmentLoader(\"PHN\"), None, ListBatcher),\n",
    "    (TIMITAlignmentLoader(\"WRD\"), None, ListBatcher),\n",
    "    (TIMITSpeakerLoader(), None, ListBatcher),\n",
    "]\n",
    "\n",
    "train_dataset = BaseDataset(\n",
    "    source=dataset.train,\n",
    "    modalities=modalities,\n",
    ")\n",
    "valid_dataset = BaseDataset(\n",
    "    source=dataset.test,\n",
    "    modalities=modalities,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, w, p, s), metadata = valid_dataset[0]\n",
    "a, w, p, s, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [valid_dataset[i] for i in range(4)]\n",
    "audio = [a for i, ((a, p, w, s), metadata) in enumerate(data)]\n",
    "x, x_sl = audio_batcher(audio)\n",
    "x = x.to(device)\n",
    "\n",
    "torch.manual_seed(6)\n",
    "loss, metrics, output = model(x, x_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio[0].shape, x.shape, output.reconstruction.shape, output.latents[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in Âµ-law space\")\n",
    "\n",
    "axes[0].plot(x[0][16000:32000].cpu())\n",
    "axes[0].plot(output.reconstruction[0][16000:32000].cpu(), alpha=0.8)\n",
    "\n",
    "axes[1].plot(x[0][21000:21500].cpu())\n",
    "axes[1].plot(output.reconstruction[0][21000:21500].cpu(), alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in linear space\")\n",
    "\n",
    "axes[0].plot(transform_dec(audio[0][16000:32000].cpu()))\n",
    "axes[0].plot(transform_dec(output.reconstruction[0][16000:32000].cpu()) + 1)\n",
    "\n",
    "axes[1].plot(transform_dec(audio[0][21500:22000].cpu()))\n",
    "axes[1].plot(transform_dec(output.reconstruction[0][21500:22000].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_component = output.reconstruction_parameters[0].softmax(-1).argmax(-1).unsqueeze(-1)\n",
    "mode = torch.gather(output.reconstruction_parameters[1], index=mode_component, dim=-1).squeeze()\n",
    "output.reconstruction_parameters[1][0].shape, mode_component.shape, mode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in linear space\")\n",
    "\n",
    "axes[0].plot(transform_dec(audio[0][16000:32000].cpu()))\n",
    "axes[0].plot(transform_dec(mode[0][16000:32000].cpu()) + 1)\n",
    "\n",
    "axes[1].plot(transform_dec(audio[0][21500:22000].cpu()))\n",
    "axes[1].plot(transform_dec(mode[0][21500:22000].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mode - x).pow(2).mean(), (output.reconstruction.squeeze() - x).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, bins, _ = plt.hist((output.reconstruction.squeeze() - x).pow(2).flatten().cpu().numpy(), alpha=0.5, bins=50, label=\"samples\")\n",
    "v, bins, _ = plt.hist((mode - x).pow(2).flatten().cpu().numpy(), alpha=0.5, bins=bins, label=\"mode\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"MSE between target and samples from or mode of $p(x_t|z)$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 21000\n",
    "\n",
    "h, w = 8, 7\n",
    "n = h * w\n",
    "\n",
    "fig, axes = plt.subplots(h, w, figsize=(16, 10), sharex=True)\n",
    "fig.tight_layout()\n",
    "axes = [a for ax in axes for a in ax]\n",
    "axes[-w//2].set_xlabel(\"$p(x_t|z^1_t,z^2_t)$ at different timesteps\", fontsize=16)\n",
    "for t in range(n):\n",
    "    plot_logistic_mixture(output.reconstruction_parameters[1][0][T+t, 0, :], output.reconstruction_parameters[2][0][T+t, 0, :], output.reconstruction_parameters[0][0][T+t, 0, :], ax=axes[t])\n",
    "    axes[t].set_title(f\"t={t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 1, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(output.reconstruction_parameters[1][0][21000:21200, 0, i].cpu(), alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different speaker, same dialect, same gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"water\"\n",
    "dialect = \"New England\"\n",
    "sex = \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, p, w, s), metadata = valid_dataset[0]\n",
    "s, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in enumerate(valid_dataset) if word in w[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in water_sentences if s.dialect == dialect and s.sex == sex]\n",
    "len(water_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, ((a, p, w, s), metadata)) in water_sentences:\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = [a for i, ((a, p, w, s), metadata) in water_sentences]\n",
    "x, x_sl = audio_batcher(audio)\n",
    "x = transform_enc(x)\n",
    "x = x.to(device)\n",
    "loss, metrics, output = model(x, x_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA of latent representations over time spanning a single work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = Isomap(n_components=2, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes, same speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA of latent representations for the same word pronounced by two different speakers.\n",
    "# Plot the PCA of latent representations for different phonemes pronounced by the same speaker.\n",
    "# Plot the PCA of latent representations for same phoneme pronounced by two different speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "052158042fcff7ad9f158fa722a81e159cf9f90b2523febac2e66a9d414ed381"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('vseq': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
