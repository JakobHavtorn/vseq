{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "import scipy.stats\n",
    "\n",
    "from vseq.data import BaseDataset\n",
    "from vseq.data.batchers import AudioBatcher, ListBatcher\n",
    "from vseq.data.datapaths import DATASETS, TIMIT\n",
    "from vseq.data.loaders import AudioLoader, TIMITAlignmentLoader, TIMITSpeakerLoader\n",
    "from vseq.data.transforms import MuLawEncode, MuLawDecode\n",
    "from vseq.models.clockwork_vae import CWVAEAudioTasNet\n",
    "from vseq.settings import CHECKPOINT_DIR\n",
    "from vseq.utils.device import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1xrnjn5y | Big V100 model, 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"1xrnjn5y\"\n",
    "dataset = DATASETS[TIMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_path(run_id, checkpoint_dir=CHECKPOINT_DIR):\n",
    "    run_dirs = os.listdir(checkpoint_dir)\n",
    "    run_dir = [run_dir for run_dir in run_dirs if run_id in run_dir]\n",
    "\n",
    "    if len(run_dir) > 1:\n",
    "        raise IOError(f\"More than one run found with ID {run_id}: {run_dir}\")\n",
    "    elif len(run_dir) == 0:\n",
    "        raise IOError(f\"No runs found with ID {run_id}\")\n",
    "    return os.path.join(checkpoint_dir, run_dir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_mixture(means, log_scales, unnorm_weights, x_min=-1, x_max=1, n_vals=100, ax=None):\n",
    "    l = np.linspace(x_min, x_max, n_vals)\n",
    "    pdf = np.zeros_like(l)\n",
    "\n",
    "    weights = unnorm_weights.softmax(-1).cpu().numpy()\n",
    "    means = means.cpu().numpy()\n",
    "    variances = log_scales.exp().cpu().numpy()\n",
    "\n",
    "    for i in range(10):\n",
    "        pdf += scipy.stats.logistic.pdf(l, loc=means[i], scale=variances[i]) * weights[i]\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "    else:\n",
    "        fig = plt.gcf()\n",
    "\n",
    "    ax.plot(l, pdf)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = get_run_path(run_id)\n",
    "run_files_path = os.path.join(run_path, \"files\")\n",
    "run_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_free_gpus()\n",
    "model = CWVAEAudioTasNet.load(run_files_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary(input_size=(4, model.overall_stride), x_sl=torch.tensor([model.overall_stride]), device=\"cpu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_loader = AudioLoader(\"wav\", cache=False)\n",
    "audio_batcher = AudioBatcher(padding_module=model.overall_stride)\n",
    "transform_enc = MuLawEncode(bits=16)\n",
    "transform_dec = MuLawDecode(bits=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = [\n",
    "    (audio_loader, transform_enc, audio_batcher),\n",
    "    (TIMITAlignmentLoader(\"PHN\"), None, ListBatcher),\n",
    "    (TIMITAlignmentLoader(\"WRD\"), None, ListBatcher),\n",
    "    (TIMITSpeakerLoader(), None, ListBatcher),\n",
    "]\n",
    "\n",
    "train_dataset = BaseDataset(\n",
    "    source=dataset.train,\n",
    "    modalities=modalities,\n",
    ")\n",
    "valid_dataset = BaseDataset(\n",
    "    source=dataset.test,\n",
    "    modalities=modalities,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, w, p, s), metadata = valid_dataset[0]\n",
    "a, w, p, s, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction accuracy is generally quite good.\n",
    "\n",
    "One can note the following things:\n",
    "- Occasionally, the sampled reconstruction for a single timestep is an outlier. This happens because a poor component is sampled from the mixture of logistics. The problem disappears when decoding from the mode.\n",
    "- The mixture weights are not generally one hot. The maximum weight has an empirical distribution with a peak at around 0.6. This leaves quite some mass to be distributed to different (to some degree) components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [valid_dataset[i] for i in range(4)]\n",
    "audio = [a for i, ((a, p, w, s), metadata) in enumerate(data)]\n",
    "x, x_sl = audio_batcher(audio)\n",
    "x = x.to(device)\n",
    "\n",
    "torch.manual_seed(6)\n",
    "loss, metrics, output = model(x, x_sl)\n",
    "data[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio[0].shape, x.shape, output.reconstruction.shape, output.latents[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in µ-law space\")\n",
    "\n",
    "axes[0].plot(x[0][16000:32000].cpu())\n",
    "axes[0].plot(output.reconstruction[0][16000:32000].cpu(), alpha=0.8)\n",
    "\n",
    "axes[1].plot(x[0][21000:21500].cpu())\n",
    "axes[1].plot(output.reconstruction[0][21000:21500].cpu(), alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in linear space\")\n",
    "\n",
    "axes[0].plot(transform_dec(audio[0][16000:32000].cpu()))\n",
    "axes[0].plot(transform_dec(output.reconstruction[0][16000:32000].cpu()) + 1)\n",
    "\n",
    "axes[1].plot(transform_dec(audio[0][21500:22000].cpu()))\n",
    "axes[1].plot(transform_dec(output.reconstruction[0][21500:22000].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_component = output.reconstruction_parameters[0].softmax(-1).argmax(-1).unsqueeze(-1)\n",
    "mode = torch.gather(output.reconstruction_parameters[1], index=mode_component, dim=-1).squeeze()\n",
    "output.reconstruction_parameters[1][0].shape, mode_component.shape, mode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in linear space\")\n",
    "\n",
    "axes[0].plot(transform_dec(audio[0][16000:32000].cpu()))\n",
    "axes[0].plot(transform_dec(mode[0][16000:32000].cpu()) + 1)\n",
    "\n",
    "axes[1].plot(transform_dec(audio[0][21500:22000].cpu()))\n",
    "axes[1].plot(transform_dec(mode[0][21500:22000].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mode - x).pow(2).mean(), (output.reconstruction.squeeze() - x).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, bins, _ = plt.hist((output.reconstruction.squeeze() - x).pow(2).flatten().cpu().numpy(), alpha=0.5, bins=50, label=\"samples\")\n",
    "v, bins, _ = plt.hist((mode - x).pow(2).flatten().cpu().numpy(), alpha=0.5, bins=bins, label=\"mode\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"MSE between target and samples from or mode of $p(x_t|z)$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the MSE high?\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in µ-law space\")\n",
    "\n",
    "axes[0].plot(x[0][16000:32000].cpu())\n",
    "axes[0].plot(output.reconstruction[0][16000:32000].cpu(), alpha=0.8)\n",
    "\n",
    "axes[1].plot(x[0][25000:25100].cpu())\n",
    "axes[1].plot(output.reconstruction[0][25000:25100].cpu(), alpha=0.8)\n",
    "\n",
    "\n",
    "abs_err = (x[0][:].cpu() - output.reconstruction[0][:].cpu().squeeze()).abs()\n",
    "axes[2].plot(abs_err)\n",
    "axes[2].plot(np.convolve(abs_err, np.ones(100)/100, mode='valid'), label=\"Running absolute error\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=512,\n",
    "    win_length=320,\n",
    "    hop_length=160,\n",
    "    power=2.0,\n",
    "    normalized=False,\n",
    "    onesided=True,\n",
    ")\n",
    "\n",
    "todb = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "spec_audio = todb(spectrogram(audio[0]))\n",
    "spec_recon = todb(spectrogram(output.reconstruction[0].squeeze().cpu()[:audio[0].shape[0]]))\n",
    "\n",
    "err_freq = (spec_audio.flip(0) - spec_recon.flip(0)).abs().mean(1)\n",
    "err_time = (spec_audio.flip(0) - spec_recon.flip(0)).abs().mean(0)\n",
    "\n",
    "spec_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(20, 10))\n",
    "axes[0].imshow(spec_audio.flip(0), aspect=\"auto\")\n",
    "axes[1].imshow(spec_recon.flip(0), aspect=\"auto\")\n",
    "axes[2].plot(err_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(err_freq[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_idx = output.reconstruction_parameters[0].softmax(-1).argmax(-1, keepdim=True)\n",
    "mode_weight = torch.gather(output.reconstruction_parameters[0].softmax(-1), index=mode_idx, dim=-1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val, va = output.reconstruction_parameters[0].softmax(-1).max(-1, keepdim=True)\n",
    "not_max_idx = output.reconstruction_parameters[0].softmax(-1) < max_val\n",
    "not_mode_weigts = output.reconstruction_parameters[0].softmax(-1)[not_max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which mixture components are used? Does it change over time?\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(output.reconstruction_parameters[0][0].squeeze().cpu().softmax(-1).T, aspect=\"auto\", interpolation=\"none\")\n",
    "axes[1].hist(mode_weight[0].cpu().numpy(), alpha=0.5, density=True, label=\"Max weight\")\n",
    "axes[1].hist(not_mode_weigts.cpu().numpy(), alpha=0.5, density=True, label=\"Other weights\");\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 21000\n",
    "\n",
    "h, w = 8, 7\n",
    "n = h * w\n",
    "\n",
    "fig, axes = plt.subplots(h, w, figsize=(16, 10), sharex=True)\n",
    "fig.tight_layout()\n",
    "axes = [a for ax in axes for a in ax]\n",
    "axes[-w//2].set_xlabel(\"$p(x_t|z^1_t,z^2_t)$ at different timesteps\", fontsize=16)\n",
    "for t in range(n):\n",
    "    plot_logistic_mixture(output.reconstruction_parameters[1][0][T+t, 0, :], output.reconstruction_parameters[2][0][T+t, 0, :], output.reconstruction_parameters[0][0][T+t, 0, :], ax=axes[t])\n",
    "    axes[t].set_title(f\"t={t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 1, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(output.reconstruction_parameters[1][0][21000:21200, 0, i].cpu(), alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different speaker, same dialect, same gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"water\"\n",
    "dialect = \"New England\"\n",
    "sex = \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, p, w, s), metadata = valid_dataset[0]\n",
    "s, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in enumerate(valid_dataset) if word in w[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in water_sentences if s.dialect == dialect and s.sex == sex]\n",
    "len(water_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, ((a, p, w, s), metadata)) in water_sentences:\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = [a for i, ((a, p, w, s), metadata) in water_sentences]\n",
    "x, x_sl = audio_batcher(audio)\n",
    "x = transform_enc(x)\n",
    "x = x.to(device)\n",
    "loss, metrics, output = model(x, x_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA of latent representations over time spanning a single work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = Isomap(n_components=2, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes, same speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA of latent representations for the same word pronounced by two different speakers.\n",
    "# Plot the PCA of latent representations for different phonemes pronounced by the same speaker.\n",
    "# Plot the PCA of latent representations for same phoneme pronounced by two different speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "052158042fcff7ad9f158fa722a81e159cf9f90b2523febac2e66a9d414ed381"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('vseq': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
