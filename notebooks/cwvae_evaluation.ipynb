{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import IPython\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "from vseq.data import BaseDataset\n",
    "from vseq.data.batchers import AudioBatcher, ListBatcher\n",
    "from vseq.data.datapaths import DATASETS, TIMIT\n",
    "from vseq.evaluation import Tracker\n",
    "from vseq.data.loaders import AudioLoader, TIMITAlignmentLoader, TIMITSpeakerLoader\n",
    "from vseq.data.samplers.batch_samplers import LengthEvalSampler\n",
    "from vseq.data.transforms import MuLawEncode, MuLawDecode\n",
    "from vseq.models.clockwork_vae import CWVAEAudioTasNet\n",
    "from vseq.settings import CHECKPOINT_DIR\n",
    "from vseq.utils.device import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1xrnjn5y | Big V100 model, 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"2k4f9q73\"  # trim-puddle, 3 layers, 8bit mu-law\n",
    "run_id = \"e4812rhg\"  # fresh-galaxy, 1 layer, 16bit mu-law\n",
    "run_id = \"1xrnjn5y\"  # sweet-sunset, 2 layers, 16bit mu-law\n",
    "dataset = DATASETS[TIMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_path(run_id, checkpoint_dir=CHECKPOINT_DIR):\n",
    "    run_dirs = os.listdir(checkpoint_dir)\n",
    "    run_dir = [run_dir for run_dir in run_dirs if run_id in run_dir]\n",
    "\n",
    "    if len(run_dir) > 1:\n",
    "        raise IOError(f\"More than one run found with ID {run_id}: {run_dir}\")\n",
    "    elif len(run_dir) == 0:\n",
    "        raise IOError(f\"No runs found with ID {run_id}\")\n",
    "    return os.path.join(checkpoint_dir, run_dir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_mixture(means, log_scales, unnorm_weights, x_min=-1, x_max=1, n_vals=100, ax=None):\n",
    "    l = np.linspace(x_min, x_max, n_vals)\n",
    "    pdf = np.zeros_like(l)\n",
    "\n",
    "    weights = unnorm_weights.softmax(-1).cpu().numpy()\n",
    "    means = means.cpu().numpy()\n",
    "    variances = log_scales.exp().cpu().numpy()\n",
    "\n",
    "    n_mix = variances.shape[0]\n",
    "\n",
    "    for i in range(n_mix):\n",
    "        pdf += scipy.stats.logistic.pdf(l, loc=means[i], scale=variances[i]) * weights[i]\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "    else:\n",
    "        fig = plt.gcf()\n",
    "\n",
    "    ax.plot(l, pdf)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = get_run_path(run_id)\n",
    "run_files_path = os.path.join(run_path, \"files\")\n",
    "run_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat\n",
    "device = get_free_gpus()\n",
    "model = CWVAEAudioTasNet.load(run_files_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary(input_size=(4, model.overall_stride), x_sl=torch.tensor([model.overall_stride]), device=\"cpu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_loader = AudioLoader(\"wav\", cache=False)\n",
    "audio_batcher = AudioBatcher(padding_module=model.overall_stride)\n",
    "transform_enc = MuLawEncode(bits=16)\n",
    "transform_dec = MuLawDecode(bits=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = [\n",
    "    (audio_loader, transform_enc, audio_batcher),\n",
    "    (TIMITAlignmentLoader(\"PHN\"), None, ListBatcher()),\n",
    "    (TIMITAlignmentLoader(\"WRD\"), None, ListBatcher()),\n",
    "    (TIMITSpeakerLoader(), None, ListBatcher()),\n",
    "]\n",
    "\n",
    "train_dataset = BaseDataset(\n",
    "    source=dataset.train,\n",
    "    modalities=modalities,\n",
    ")\n",
    "valid_dataset = BaseDataset(\n",
    "    source=dataset.test,\n",
    "    modalities=modalities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, w, p, s), metadata = valid_dataset[0]\n",
    "# a, w, p, s, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent space evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from vseq.data.loaders import TIMIT_PHONE2INT, TIMIT_INT2PHONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different phonemes, same speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, p, w, s), metadata = valid_dataset[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = \"AKS0\"\n",
    "speaker_id = \"BJK0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_data = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in enumerate(valid_dataset) if s.speaker_id == speaker_id]\n",
    "print(len(speaker_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_audio = [a for (i, ((a, p, w, s), metadata)) in speaker_data]\n",
    "speaker_phone = [p for (i, ((a, p, w, s), metadata)) in speaker_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_phone_token = [p[2] for (i, ((a, p, w, s), metadata)) in speaker_data]\n",
    "speaker_phone_start = [torch.tensor(p[0]) for (i, ((a, p, w, s), metadata)) in speaker_data]\n",
    "speaker_phone_stop = [torch.tensor(p[1]) for (i, ((a, p, w, s), metadata)) in speaker_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count most common phonemes\n",
    "all_speaker_phones = [p for phs in speaker_phone_token for p in phs ]\n",
    "cntr = Counter(all_speaker_phones)\n",
    "print(len(all_speaker_phones), \"\\n\", cntr.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to time scale of latent representations\n",
    "speaker_phone_start_latent = [[torch.floor(p / tf).to(int) for p in speaker_phone_start] for tf in model.time_factors]\n",
    "speaker_phone_stop_latent = [[torch.ceil(p / tf).to(int) for p in speaker_phone_stop] for tf in model.time_factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_phone_start_latent[0][0][-1], speaker_phone_start_latent[1][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align labels with latent representations\n",
    "labels_latent = [None] * n_latents\n",
    "labels_length = [None] * n_latents\n",
    "for i_latent in range(n_latents):\n",
    "    labels_latent[i_latent] = []\n",
    "    labels_length[i_latent] = []\n",
    "\n",
    "    for i_example in range(n_examples):\n",
    "        length = speaker_phone_stop_latent[i_latent][i_example].shape[-1]\n",
    "\n",
    "        boundaries = torch.cat([torch.zeros(1), speaker_phone_stop_latent[i_latent][i_example]])\n",
    "\n",
    "        labels = []\n",
    "        for i_element in range(length):\n",
    "            start = int(boundaries[i_element])\n",
    "            stop = int(boundaries[i_element+1])\n",
    "\n",
    "            labs = [TIMIT_PHONE2INT[speaker_phone_token[i_example][i_element]]] * (stop - start)\n",
    "            labels.extend(labs)\n",
    "\n",
    "        labels_length[i_latent].append(len(labels))\n",
    "        labels_latent[i_latent].append(torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed in latent space via model\n",
    "outputs = []\n",
    "for x in speaker_audio:\n",
    "    x, x_sl = audio_batcher([x])\n",
    "    x = x.to(device)\n",
    "    loss, metrics, output = model(x, x_sl)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].latents[0].shape, outputs[0].latents[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latents = len(outputs[0].latents)\n",
    "n_examples = len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latents[i_latent][i_example] squeeze batch dim\n",
    "latents = [[outputs[b].latents[i_latent].squeeze(0) for b in range(n_examples)] for i_latent in range(n_latents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove padding from latent representations according to phoneme labels\n",
    "print(latents[0][1].shape)\n",
    "latents = [[latents[i_latent][i_example][:labels_length[i_latent][i_example]] for i_example in range(n_examples)] for i_latent in range(n_latents)]\n",
    "print(latents[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all representations as one timeseries per latent\n",
    "latents_cat = [torch.cat(latents[i_latent], dim=0) for i_latent in range(n_latents)]\n",
    "labels_latent_cat = [torch.cat(labels_latent[i_latent], dim=0) for i_latent in range(n_latents)]\n",
    "latents_cat[0].shape, labels_latent_cat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 7\n",
    "combinations = list(itertools.combinations(range(n_components), 2))\n",
    "n_combinations = len(combinations)\n",
    "print(n_combinations, np.sqrt(n_combinations), int(np.ceil(np.sqrt(n_combinations))), combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3\n",
    "w = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components, whiten=True)\n",
    "\n",
    "latents_pca = [pca.fit_transform(latents_cat[i_latent].cpu().numpy()) for i_latent in range(n_latents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = Isomap(n_components=n_components, n_neighbors=5)\n",
    "\n",
    "latents_isomap = [isomap.fit_transform(latents_cat[i_latent].cpu().numpy()) for i_latent in range(n_latents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[('h#', 20), ('n', 20), ('ix', 19), ('s', 18), ('iy', 16), ('kcl', 15), ('r', 14), ('k', 14), ('l', 13), ('d', 12), ('tcl', 11), ('ax', 11), ('bcl', 11), ('dcl', 10), ('aa', 10), ('b', 10), ('f', 10), ('er', 9), ('ae', 8), ('sh', 7), ('ih', 7), ('w', 7), ('ao', 6), ('axr', 6), ('t', 6), ('dh', 6), ('z', 6), ('p', 6), ('y', 5), ('gcl', 5), ('g', 5), ('epi', 5), ('ow', 5), ('pcl', 5), ('q', 4), ('m', 4), ('v', 4), ('uw', 3), ('dx', 3), ('ay', 3), ('ux', 3), ('el', 3), ('aw', 3), ('ey', 3), ('hv', 2), ('eh', 2), ('hh', 2), ('ah', 2), ('en', 2), ('ch', 2), ('uh', 2), ('ng', 1), ('oy', 1), ('th', 1)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_PHONEMES = [\"iy\", \"n\"]\n",
    "SELECTED_PHONEMES = [\"iy\", \"jh\"]\n",
    "SELECTED_PHONEMES = [\"hh\", \"ng\"]\n",
    "SELECTED_PHONEMES = [\"s\", \"y\"]\n",
    "# SELECTED_PHONEMES = [\"iy\", \"ih\", \"eh\", \"ey\", \"ae\", \"aa\", \"aw\", \"ay\", \"ah\", \"ao\", \"oy\", \"ow\", \"uh\", \"uw\", \"ux\", \"er\", \"ax\", \"ix\", \"axr\", \"ax-h\"]\n",
    "\n",
    "phonemes_int = [TIMIT_PHONE2INT[p] for p in SELECTED_PHONEMES]\n",
    "\n",
    "phonemes2idx = [dict([(p, labels_latent_cat[i_latent] == pi) for p, pi in zip(SELECTED_PHONEMES, phonemes_int)]) for i_latent in range(n_latents)]\n",
    "\n",
    "idx_or = [sum([idx for idx in phonemes2idx[i_latent].values()]).to(bool) for i_latent in range(n_latents)]  # select all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_latent = 0\n",
    "\n",
    "fig, axes = plt.subplots(h, w, figsize=(16, 9))\n",
    "fig.tight_layout()\n",
    "\n",
    "axes = [a for ax in axes for a in ax]\n",
    "for c in range(n_combinations):\n",
    "    for phone, idx in phonemes2idx[i_latent].items():\n",
    "        i, j = combinations[c]\n",
    "        axes[c].scatter(latents_pca[i_latent][idx, i], latents_pca[i_latent][idx, j], alpha=0.5, label=phone)\n",
    "        # axes[c].scatter(latents_pca[i_latent][:, i], latents_pca[i_latent][:, j], alpha=0.5, label=phone)\n",
    "        axes[c].set_xlabel(i)\n",
    "        axes[c].set_ylabel(j)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_latent = 0\n",
    "\n",
    "fig, axes = plt.subplots(h, w, figsize=(16, 9))\n",
    "fig.tight_layout()\n",
    "\n",
    "axes = [a for ax in axes for a in ax]\n",
    "for c in range(n_combinations):\n",
    "    for phone, idx in phonemes2idx[i_latent].items():\n",
    "        i, j = combinations[c]\n",
    "        axes[c].scatter(latents_isomap[i_latent][idx, i], latents_isomap[i_latent][idx, j], alpha=0.5, label=phone)\n",
    "        # axes[c].scatter(latents_pca[i_latent][:, i], latents_pca[i_latent][:, j], alpha=0.5, label=phone)\n",
    "        axes[c].set_xlabel(i)\n",
    "        axes[c].set_ylabel(j)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe with SVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "scaler = StandardScaler()\n",
    "clf = SVC(kernel=\"linear\", C=0.025)\n",
    "\n",
    "idx_or = sum([idx for idx in phonemes2idx[i_latent].values()]).to(bool)  # select all classes\n",
    "\n",
    "# x = latents_cat[i_latent].cpu().numpy()[idx_or]\n",
    "x = latents_pca[i_latent][idx_or]\n",
    "y = labels_latent_cat[i_latent].cpu().numpy()[idx_or]\n",
    "\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.25, random_state=0)\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "score_train = clf.score(x_train, y_train)\n",
    "score_test = clf.score(x_test, y_test)\n",
    "print(score_train, score_test, np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(latents_isomap[0][:, 0], latents_isomap[0][:, 1])\n",
    "fig.savefig(f\"./figures/cwvae_evaluation/{run_id}_latent_0_isomap.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.scatter(latents_isomap[1][:, 0], latents_isomap[1][:, 1])\n",
    "fig.savefig(f\"./figures/cwvae_evaluation/{run_id}_latent_0_isomap.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, p, w, s), metadata = valid_dataset[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in enumerate(valid_dataset) if word in w[2]]\n",
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in water_sentences if s.dialect == dialect and s.sex == sex]\n",
    "len(water_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sampler = LengthEvalSampler(\n",
    "    source=dataset.test,\n",
    "    field=\"length.wav.samples\",\n",
    "    max_len=\"max\",\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    collate_fn=valid_dataset.collate,\n",
    "    num_workers=4,\n",
    "    batch_sampler=valid_sampler,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "outputs = []\n",
    "\n",
    "tracker = Tracker(print_every=None)\n",
    "with torch.no_grad():\n",
    "    # (a, p, w, s), metadata\n",
    "    for ((x, x_sl), (p, p_sl), (w, w_sl), (s, s_sl)), metadata in tracker(valid_loader):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "\n",
    "        loss, metrics, output = model(x, x_sl)\n",
    "\n",
    "        tracker.update(metrics)\n",
    "\n",
    "        outputs.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different speaker, same dialect, same gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"water\"\n",
    "dialect = \"New England\"\n",
    "sex = \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, p, w, s), metadata = valid_dataset[0]\n",
    "s, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in enumerate(valid_dataset) if word in w[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in water_sentences if s.dialect == dialect and s.sex == sex]\n",
    "len(water_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, ((a, p, w, s), metadata)) in water_sentences:\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = [a for i, ((a, p, w, s), metadata) in water_sentences]\n",
    "x, x_sl = audio_batcher(audio)\n",
    "x = transform_enc(x)\n",
    "x = x.to(device)\n",
    "loss, metrics, output = model(x, x_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.reconstructions.shape, output.latents[0].shape, output.latents[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = Isomap(n_components=2, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, p, w, s), metadata = valid_dataset[0]\n",
    "s, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in enumerate(valid_dataset) if word in w[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in water_sentences if s.dialect == dialect and s.sex == sex]\n",
    "len(water_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, ((a, p, w, s), metadata)) in water_sentences:\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = [a for i, ((a, p, w, s), metadata) in water_sentences]\n",
    "x, x_sl = audio_batcher(audio)\n",
    "x = transform_enc(x)\n",
    "x = x.to(device)\n",
    "loss, metrics, output = model(x, x_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.reconstructions.shape, output.latents[0].shape, output.latents[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = Isomap(n_components=2, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different speaker, same dialect, same gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"water\"\n",
    "dialect = \"New England\"\n",
    "sex = \"F\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, p, w, s), metadata = valid_dataset[0]\n",
    "s, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in enumerate(valid_dataset) if word in w[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_sentences = [(i, ((a, p, w, s), metadata)) for i, ((a, p, w, s), metadata) in water_sentences if s.dialect == dialect and s.sex == sex]\n",
    "len(water_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, ((a, p, w, s), metadata)) in water_sentences:\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = [a for i, ((a, p, w, s), metadata) in water_sentences]\n",
    "x, x_sl = audio_batcher(audio)\n",
    "x = transform_enc(x)\n",
    "x = x.to(device)\n",
    "loss, metrics, output = model(x, x_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.reconstructions.shape, output.latents[0].shape, output.latents[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = Isomap(n_components=2, n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemes, same speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA of latent representations for the same word pronounced by two different speakers.\n",
    "# Plot the PCA of latent representations for different phonemes pronounced by the same speaker.\n",
    "# Plot the PCA of latent representations for same phoneme pronounced by two different speakers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "Reconstruction accuracy is generally quite good.\n",
    "\n",
    "One can note the following things:\n",
    "- Occasionally, the sampled reconstruction for a single timestep is an outlier. This happens because a poor component is sampled from the mixture of logistics. The problem disappears when decoding from the mode.\n",
    "- The mixture weights are not generally one hot. The maximum weight has an empirical distribution with a peak at around 0.6. This leaves quite some mass to be distributed to different (to some degree) components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [valid_dataset[i] for i in range(4)]\n",
    "audio = [a for i, ((a, p, w, s), metadata) in enumerate(data)]\n",
    "x, x_sl = audio_batcher(audio)\n",
    "x = x.to(device)\n",
    "\n",
    "torch.manual_seed(6)\n",
    "loss, metrics, output = model(x, x_sl)\n",
    "data[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio[0].shape, x.shape, output.reconstructions.shape, output.latents[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in µ-law space\")\n",
    "\n",
    "axes[0].plot(x[0][16000:32000].cpu())\n",
    "axes[0].plot(output.reconstructions[0][16000:32000].cpu(), alpha=0.8)\n",
    "\n",
    "axes[1].plot(x[0][21000:21500].cpu())\n",
    "axes[1].plot(output.reconstructions[0][21000:21500].cpu(), alpha=0.8)\n",
    "\n",
    "fig.savefig(f\"./figures/cwvae_evaluation/{run_id}_reconstruction_mu_law.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in linear space\")\n",
    "\n",
    "axes[0].plot(transform_dec(audio[0][16000:32000].cpu()))\n",
    "axes[0].plot(transform_dec(output.reconstructions[0][16000:32000].cpu()) + 1)\n",
    "\n",
    "axes[1].plot(transform_dec(audio[0][21500:22000].cpu()))\n",
    "axes[1].plot(transform_dec(output.reconstructions[0][21500:22000].cpu()))\n",
    "\n",
    "fig.savefig(f\"./figures/cwvae_evaluation/{run_id}_reconstruction_linear.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_component = output.reconstructions_parameters[0].softmax(-1).argmax(-1).unsqueeze(-1)\n",
    "mode = torch.gather(output.reconstructions_parameters[1], index=mode_component, dim=-1).squeeze()\n",
    "output.reconstructions_parameters[1][0].shape, mode_component.shape, mode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in linear space\")\n",
    "\n",
    "axes[0].plot(transform_dec(audio[0][16000:32000].cpu()))\n",
    "axes[0].plot(transform_dec(mode[0][16000:32000].cpu()) + 1)\n",
    "\n",
    "axes[1].plot(transform_dec(audio[0][21500:22000].cpu()))\n",
    "axes[1].plot(transform_dec(mode[0][21500:22000].cpu()))\n",
    "\n",
    "fig.savefig(f\"./figures/cwvae_evaluation/{run_id}_reconstruction_linear_from_mode.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mode - x).pow(2).mean(), (output.reconstructions.squeeze() - x).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, bins, _ = plt.hist((output.reconstructions.squeeze() - x).pow(2).flatten().cpu().numpy(), alpha=0.5, bins=50, label=\"samples\")\n",
    "v, bins, _ = plt.hist((mode - x).pow(2).flatten().cpu().numpy(), alpha=0.5, bins=bins, label=\"mode\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"MSE between target and samples or mode of $p(x_t|z)$\")\n",
    "plt.legend()\n",
    "fig.savefig(f\"./figures/cwvae_evaluation/{run_id}_reconstruction_linear_mse_samples_mode.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the MSE high?\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in µ-law space\")\n",
    "\n",
    "axes[0].plot(x[0][16000:32000].cpu())\n",
    "axes[0].plot(output.reconstructions[0][16000:32000].cpu(), alpha=0.8)\n",
    "\n",
    "axes[1].plot(x[0][25000:25100].cpu())\n",
    "axes[1].plot(output.reconstructions[0][25000:25100].cpu(), alpha=0.8)\n",
    "\n",
    "\n",
    "abs_err = (x[0][:].cpu() - output.reconstructions[0][:].cpu().squeeze()).abs()\n",
    "axes[2].plot(abs_err, label=\"Absolute error\")\n",
    "axes[2].plot(np.convolve(abs_err, np.ones(100)/100, mode='valid'), label=\"Running (100) absolute error\")\n",
    "\n",
    "plt.legend()\n",
    "fig.savefig(f\"./figures/cwvae_evaluation/{run_id}_reconstruction_linear_abs_err_over_time.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=512,\n",
    "    win_length=320,\n",
    "    hop_length=160,\n",
    "    power=2.0,\n",
    "    normalized=False,\n",
    "    onesided=True,\n",
    ")\n",
    "\n",
    "todb = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "spec_audio = todb(spectrogram(audio[0]))\n",
    "spec_recon = todb(spectrogram(output.reconstructions[0].squeeze().cpu()[:audio[0].shape[0]]))\n",
    "\n",
    "err_freq = (spec_audio.flip(0) - spec_recon.flip(0)).abs().mean(1)\n",
    "err_time = (spec_audio.flip(0) - spec_recon.flip(0)).abs().mean(0)\n",
    "\n",
    "spec_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(20, 10))\n",
    "axes[0].imshow(spec_audio.flip(0), aspect=\"auto\")\n",
    "axes[1].imshow(spec_recon.flip(0), aspect=\"auto\")\n",
    "axes[2].plot(err_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(err_freq[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_idx = output.reconstructions_parameters[0].softmax(-1).argmax(-1, keepdim=True)\n",
    "mode_weight = torch.gather(output.reconstructions_parameters[0].softmax(-1), index=mode_idx, dim=-1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val, va = output.reconstructions_parameters[0].softmax(-1).max(-1, keepdim=True)\n",
    "not_max_idx = output.reconstructions_parameters[0].softmax(-1) < max_val\n",
    "not_mode_weigts = output.reconstructions_parameters[0].softmax(-1)[not_max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which mixture components are used? Does it change over time?\n",
    "if model.num_mix > 1:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axes[0].imshow(output.reconstructions_parameters[0][0].squeeze().cpu().softmax(-1).T, aspect=\"auto\", interpolation=\"none\")\n",
    "    axes[1].hist(mode_weight[0].cpu().numpy(), alpha=0.5, density=True, label=\"Max weight\")\n",
    "    axes[1].hist(not_mode_weigts.cpu().numpy(), alpha=0.5, density=True, label=\"Other weights\")\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.num_mix > 1:\n",
    "    fig, axes = plt.subplots(10, 1, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.plot(output.reconstructions_parameters[1][0][21000:21200, 0, i].cpu(), alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 21000\n",
    "\n",
    "h, w = 8, 7\n",
    "n = h * w\n",
    "\n",
    "fig, axes = plt.subplots(h, w, figsize=(16, 10), sharex=True)\n",
    "fig.tight_layout()\n",
    "axes = [a for ax in axes for a in ax]\n",
    "axes[-w//2].set_xlabel(\"$p(x_t|z^1_t,z^2_t)$ at different timesteps\", fontsize=16)\n",
    "for t in range(n):\n",
    "    plot_logistic_mixture(output.reconstructions_parameters[1][0][T+t, 0, :], output.reconstructions_parameters[2][0][T+t, 0, :], output.reconstructions_parameters[0][0][T+t, 0, :], ax=axes[t])\n",
    "    axes[t].set_title(f\"t={t}\")\n",
    "\n",
    "fig.savefig(f\"./figures/cwvae_evaluation/{run_id}_reconstruction_p_x_distributions_over_time.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction accuracy is generally quite good.\n",
    "\n",
    "One can note the following things:\n",
    "- Occasionally, the sampled reconstruction for a single timestep is an outlier. This happens because a poor component is sampled from the mixture of logistics. The problem disappears when decoding from the mode.\n",
    "- The mixture weights are not generally one hot. The maximum weight has an empirical distribution with a peak at around 0.6. This leaves quite some mass to be distributed to different (to some degree) components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [valid_dataset[i] for i in range(4)]\n",
    "audio = [a for i, ((a, p, w, s), metadata) in enumerate(data)]\n",
    "x, x_sl = audio_batcher(audio)\n",
    "x = x.to(device)\n",
    "\n",
    "torch.manual_seed(6)\n",
    "loss, metrics, output = model(x, x_sl)\n",
    "data[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio[0].shape, x.shape, output.reconstructions.shape, output.latents[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in µ-law space\")\n",
    "\n",
    "axes[0].plot(x[0][16000:32000].cpu())\n",
    "axes[0].plot(output.reconstructions[0][16000:32000].cpu(), alpha=0.8)\n",
    "\n",
    "axes[1].plot(x[0][21000:21500].cpu())\n",
    "axes[1].plot(output.reconstructions[0][21000:21500].cpu(), alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in linear space\")\n",
    "\n",
    "axes[0].plot(transform_dec(audio[0][16000:32000].cpu()))\n",
    "axes[0].plot(transform_dec(output.reconstructions[0][16000:32000].cpu()) + 1)\n",
    "\n",
    "axes[1].plot(transform_dec(audio[0][21500:22000].cpu()))\n",
    "axes[1].plot(transform_dec(output.reconstructions[0][21500:22000].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_component = output.reconstructions_parameters[0].softmax(-1).argmax(-1).unsqueeze(-1)\n",
    "mode = torch.gather(output.reconstructions_parameters[1], index=mode_component, dim=-1).squeeze()\n",
    "output.reconstructions_parameters[1][0].shape, mode_component.shape, mode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in linear space\")\n",
    "\n",
    "axes[0].plot(transform_dec(audio[0][16000:32000].cpu()))\n",
    "axes[0].plot(transform_dec(mode[0][16000:32000].cpu()) + 1)\n",
    "\n",
    "axes[1].plot(transform_dec(audio[0][21500:22000].cpu()))\n",
    "axes[1].plot(transform_dec(mode[0][21500:22000].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mode - x).pow(2).mean(), (output.reconstructions.squeeze() - x).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, bins, _ = plt.hist((output.reconstructions.squeeze() - x).pow(2).flatten().cpu().numpy(), alpha=0.5, bins=50, label=\"samples\")\n",
    "v, bins, _ = plt.hist((mode - x).pow(2).flatten().cpu().numpy(), alpha=0.5, bins=bins, label=\"mode\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"MSE between target and samples from or mode of $p(x_t|z)$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the MSE high?\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(20, 10))\n",
    "\n",
    "axes[0].set_title(\"Reconstructions in µ-law space\")\n",
    "\n",
    "axes[0].plot(x[0][16000:32000].cpu())\n",
    "axes[0].plot(output.reconstructions[0][16000:32000].cpu(), alpha=0.8)\n",
    "\n",
    "axes[1].plot(x[0][25000:25100].cpu())\n",
    "axes[1].plot(output.reconstructions[0][25000:25100].cpu(), alpha=0.8)\n",
    "\n",
    "\n",
    "abs_err = (x[0][:].cpu() - output.reconstructions[0][:].cpu().squeeze()).abs()\n",
    "axes[2].plot(abs_err)\n",
    "axes[2].plot(np.convolve(abs_err, np.ones(100)/100, mode='valid'), label=\"Running absolute error\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=512,\n",
    "    win_length=320,\n",
    "    hop_length=160,\n",
    "    power=2.0,\n",
    "    normalized=False,\n",
    "    onesided=True,\n",
    ")\n",
    "\n",
    "todb = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "spec_audio = todb(spectrogram(audio[0]))\n",
    "spec_recon = todb(spectrogram(output.reconstructions[0].squeeze().cpu()[:audio[0].shape[0]]))\n",
    "\n",
    "err_freq = (spec_audio.flip(0) - spec_recon.flip(0)).abs().mean(1)\n",
    "err_time = (spec_audio.flip(0) - spec_recon.flip(0)).abs().mean(0)\n",
    "\n",
    "spec_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(20, 10))\n",
    "axes[0].imshow(spec_audio.flip(0), aspect=\"auto\")\n",
    "axes[1].imshow(spec_recon.flip(0), aspect=\"auto\")\n",
    "axes[2].plot(err_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(err_freq[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_idx = output.reconstructions_parameters[0].softmax(-1).argmax(-1, keepdim=True)\n",
    "mode_weight = torch.gather(output.reconstructions_parameters[0].softmax(-1), index=mode_idx, dim=-1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val, va = output.reconstructions_parameters[0].softmax(-1).max(-1, keepdim=True)\n",
    "not_max_idx = output.reconstructions_parameters[0].softmax(-1) < max_val\n",
    "not_mode_weigts = output.reconstructions_parameters[0].softmax(-1)[not_max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which mixture components are used? Does it change over time?\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(output.reconstructions_parameters[0][0].squeeze().cpu().softmax(-1).T, aspect=\"auto\", interpolation=\"none\")\n",
    "axes[1].hist(mode_weight[0].cpu().numpy(), alpha=0.5, density=True, label=\"Max weight\")\n",
    "axes[1].hist(not_mode_weigts.cpu().numpy(), alpha=0.5, density=True, label=\"Other weights\");\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 21000\n",
    "\n",
    "h, w = 8, 7\n",
    "n = h * w\n",
    "\n",
    "fig, axes = plt.subplots(h, w, figsize=(16, 10), sharex=True)\n",
    "fig.tight_layout()\n",
    "axes = [a for ax in axes for a in ax]\n",
    "axes[-w//2].set_xlabel(\"$p(x_t|z^1_t,z^2_t)$ at different timesteps\", fontsize=16)\n",
    "for t in range(n):\n",
    "    plot_logistic_mixture(output.reconstructions_parameters[1][0][T+t, 0, :], output.reconstructions_parameters[2][0][T+t, 0, :], output.reconstructions_parameters[0][0][T+t, 0, :], ax=axes[t])\n",
    "    axes[t].set_title(f\"t={t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 1, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(output.reconstructions_parameters[1][0][21000:21200, 0, i].cpu(), alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "052158042fcff7ad9f158fa722a81e159cf9f90b2523febac2e66a9d414ed381"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('vseq': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
